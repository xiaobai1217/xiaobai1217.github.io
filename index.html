<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Yunhua Zhang</title>
  
  <meta name="author" content="Yunhua Zhang">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Yunhua Zhang</name>
              </p>
              <p> I am a PhD candidate at University of Amsterdam (UvA), supervised by Prof. <a href="https://www.ceessnoek.info/">Cees Snoek</a>. 
              </p>

              <p style="text-align:center">
                <a href="mailto:y.zhang9@uva.nl">Email</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=Yrw15pUAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/xiaobai1217/">Github</a> &nbsp/&nbsp 
                <a href="https://ellis.eu/projects/multimodal-video-learning">ELLIS PhD Program</a>
              </p>
              <p style="text-align:center">
                <a href="https://github.com/xiaobai1217/Awesome-Video-Datasets">Awesome-Video-Datasets</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/Yunhua.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/Yunhua_circle.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                I'm interested in computer vision, video understanding, audiovisual learning and object tracking.
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/cvpr2022.png" alt="blind-date" width="160" height="160">
            </td>
            <td width="75%" valign="middle">
                <papertitle>Audio-Adaptive Activity Recognition Across Video Domains</papertitle>
              </a>
              <br>
              <strong>Yunhua Zhang</strong>, Hazel Doughty, Ling Shao, Cees G.M. Snoek
              <br>
              <em>CVPR</em>, 2022
              <br>
                <a href="data/cvpr2022.bib">bibtex</a> / <a href="https://arxiv.org/abs/2203.14240">arXiv</a> / <a href="https://github.com/xiaobai1217/DomainAdaptation">code &amp; data</a> / <a href="https://xiaobai1217.github.io/DomainAdaptation/">website</a> / <a href="https://youtu.be/Lh3gb6NMhB4">demo video</a> / <a href="https://drive.google.com/file/d/1xdWB6dJvZ5zUPtChCA5TcxaAWPCIm4Gw/view?usp=sharing">CVPR presentation</a>
               <br>
              <font color="red"><strong>2nd place in the UDA track, <a href="https://www.youtube.com/watch?v=kLRn-Q48hr0&t=3131s">EPIC-Kitchens Challenge @CVPR 2022</a>. </strong></font>
              <br>
              <p>This paper strives for activity recognition under domain shift, for example caused by change of scenery or camera viewpoint. Different from vision-focused works we leverage activity sounds for domain adaptation as they have less variance across domains and can reliably indicate which activities are not happening. </p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/repetition.jpg" alt="blind-date" width="160" height="160">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/pdf/2103.13096.pdf">
                <papertitle>Repetitive Activity Counting by Sight and Sound</papertitle>
              </a>
              <br>
              <strong>Yunhua Zhang</strong>, Ling Shao, Cees G.M. Snoek
              <br>
              <em>CVPR</em>, 2021
              <br>
                <a href="data/repetition.bib">bibtex</a> / <a href="https://arxiv.org/pdf/2103.13096.pdf">arXiv</a> / <a href="https://user-images.githubusercontent.com/22721775/112766700-2c7b7980-9013-11eb-8667-95ce6ec31067.mp4">demo video</a> / <a href="https://drive.google.com/file/d/10n3SuvPM5d2YGUbMYxZpeo1rLhaOwQ3O/view?usp=sharing">CVPR presentation</a> / <a href="https://github.com/xiaobai1217/RepetitionCounting">code &amp; data</a>
               <br>
               Also presented at <a href="https://eyewear-computing.org/EPIC_CVPR21/program">EPIC Workshop@CVPR 2021</a> and <a href="https://sightsound.org/#papers">Sight and Sound Workshop@CVPR 2021</a>
              <p>We incorporate for the first time the corresponding sound into the repetition counting process. This benefits accuracy in challenging vision conditions such as occlusion, dramatic camera view changes, low resolution, etc.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/IJCV2021.png" alt="blind-date" width="160" height="100">
            </td>
            <td width="75%" valign="middle">
              <a href="https://link.springer.com/content/pdf/10.1007/s11263-021-01487-3.pdf">
                <papertitle>Learning Regression and Verification Networks for Robust Long-term
Tracking</papertitle>
              </a>
              <br>
              <strong>Yunhua Zhang</strong>, Lijun Wang, Dong Wang, Jinqing Qi, Huchuan Lu
              <br>
              <em>IJCV</em>, 2021
              <br>
                <a href="data/ijcv2021.bib">bibtex</a> / <a href="https://link.springer.com/content/pdf/10.1007/s11263-021-01487-3.pdf">PDF</a> / <a href="https://github.com/xiaobai1217/MBMD">code</a>
              <p>This paper proposes a new visual tracking algorithm, which leverages the merits of both template matching approaches and classification models for long-term object detection and tracking.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/meta_updater.png" alt="blind-date" width="160" height="100">
            </td>
            <td width="75%" valign="middle">
              <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Dai_High-Performance_Long-Term_Tracking_With_Meta-Updater_CVPR_2020_paper.pdf">
                <papertitle>High-Performance Long-Term Tracking with Meta-Updater</papertitle>
              </a>
              <br>
              Kenan Dai, <strong>Yunhua Zhang</strong>, Dong Wang, Jianhua Li, Huchuan Lu, Xiaoyun Yang
              <br>
              <em>CVPR</em>, 2020 &nbsp <font color="red"><strong>(Best Paper Award Nominee)</strong></font>
              <br>
                <a href="data/cvpr2020.bib">bibtex</a> / <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Dai_High-Performance_Long-Term_Tracking_With_Meta-Updater_CVPR_2020_paper.pdf">PDF</a> / <a href="https://github.com/Daikenan/LTMU">code</a>
              <p>In this work, we propose a novel offline-trained Meta-Updater to address an important but unsolved problem: Is the tracker ready for updating in the current frame?</p>
            </td>
          </tr>

            <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/eccv2018.png" alt="blind-date" width="160" height="100">
            </td>
            <td width="75%" valign="middle">
              <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Dai_High-Performance_Long-Term_Tracking_With_Meta-Updater_CVPR_2020_paper.pdf">
                <papertitle>Structured Siamese Network for Real-Time Visual Tracking</papertitle>
              </a>
              <br>
              <strong>Yunhua Zhang</strong>, Lijun Wang, Jinqing Qi, Dong Wang, Mengyang Feng, Huchuan Lu
              <br>
              <em>ECCV</em>, 2018 
              <br>
                <a href="data/eccv2018.bib">bibtex</a> / <a href="https://openaccess.thecvf.com/content_ECCV_2018/papers/Yunhua_Zhang_Structured_Siamese_Network_ECCV_2018_paper.pdf">PDF</a> </a>
              <p>In this paper, we propose a local structure learning method, which simultaneously considers the local patterns of the target and their structural relationships for more accurate target tracking.</p>
            </td>
          </tr>

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Challenges</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/vot.png" width="120" height="160"></td>
            <td width="75%" valign="center">
            	The winning track of VOT2019 Long-term Tracking Challenge
            	<br>
            	Kenan Dai, <strong>Yunhua Zhang</strong>, Jianhua Li, Dong Wang, Xiaoyun Yang, Huchuan Lu
            	<br>
            	<em>ICCV Workshop</em>, 2019
            	<br>
              <a href="https://openaccess.thecvf.com/content_ECCVW_2018/papers/11129/Kristan_The_sixth_Visual_Object_Tracking_VOT2018_challenge_results_ECCVW_2018_paper.pdf">Report</a> / <a href="https://github.com/Daikenan/LT_DSE">Code</a>
              <br><br>
              The winning track of VOT2018 Long-term Tracking Challenge
              <br>
              <strong>Yunhua Zhang</strong>, Lijun Wang, Dong Wang, JinQing Qi, Huchuan Lu
              <br>
              <em>ECCV Workshop</em>, 2018
              <br>
              <a href="https://openaccess.thecvf.com/content_ICCVW_2019/papers/VOT/Kristan_The_Seventh_Visual_Object_Tracking_VOT2019_Challenge_Results_ICCVW_2019_paper.pdf">Report</a> / <a href="https://github.com/xiaobai1217/MBMD">Code</a>
            </td>
          </tr>




        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:left;font-size:small;">
                This website uses the template provided by <a href="https://jonbarron.info/">Jon Barron</a>. 
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
